{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukslima/6out/blob/main/11_exerc_cios_mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Exercício 1 - Problema do XOR\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 1, 1, 0])\n",
        "\n",
        "# 1- Importar o modelo MLP do sklearn\n",
        "\n",
        "# 2- Instanciar o modelo escolhendo uma topologia para a rede, função de ativação e número de épocas de execução até 100% de acerto\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(2,), activation='relu', max_iter=10000)\n",
        "\n",
        "# 3- Treinar o modelo com os dados de treinamento\n",
        "mlp.fit(x, y)\n",
        "\n",
        "# 4- Fazer o predict com os dados de treinamento\n",
        "y_pred = mlp.predict(x)\n",
        "\n",
        "# 5- Comparar o resultado do predict com o vetor y\n",
        "print(\"Previsões:\", y_pred)\n",
        "print(\"Verdadeiros valores:\", y)\n",
        "print(\"Acurácia:\", mlp.score(x, y))\n",
        "\n",
        "# 6- Imprimir o percentual de acerto da base de treinamento"
      ],
      "metadata": {
        "id": "Q8khw7gpctj0",
        "outputId": "f75bda0a-5f2c-4921-e544-8cf0b8680049",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previsões: [1 1 0 0]\n",
            "Verdadeiros valores: [0 1 1 0]\n",
            "Acurácia: 0.5\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# 1- Carregar a base de dados Parkinsons\n",
        "url = \"https://raw.githubusercontent.com/tmoura/machinelearning/master/parkinsons.data\"\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# 2- Normalizar todas as colunas (normalizar é deixar todos os valores das colunas entre 0 e 1)\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Get a list of all columns except the target column ('status')\n",
        "columns_to_scale = [col for col in data.columns if col != 'status']\n",
        "\n",
        "# Scale the selected columns\n",
        "data_scaled = scaler.fit_transform(data[columns_to_scale])\n",
        "\n",
        "# Create a DataFrame with the scaled data and original column names\n",
        "data_scaled = pd.DataFrame(data_scaled, columns=columns_to_scale, index=data.index)\n",
        "\n",
        "# 3- Separar o dataset em X (matriz de features) e y (coluna target)\n",
        "X = data_scaled\n",
        "y = data['status']\n",
        "\n",
        "# 4- Gerar as bases de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 5- Importar o modelo MLP do sklearn\n",
        "\n",
        "# 6- Instanciar o modelo escolhendo uma topologia para a rede, função de ativação e número de épocas de execução até que obtenha uma taxa de acerto estável\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(10,), activation='relu', max_iter=1000, random_state=42)\n",
        "\n",
        "# 7- Treinar o modelo com os dados de treinamento\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# 8- Fazer o predict com os dados de teste\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "# 9- Imprimir o percentual de acerto da base de teste\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(f\"Acurácia na base de teste: {accuracy * 100:.2f}%\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "W8bJmSy96gBT",
        "outputId": "97661f67-0011-4b0d-fd67-931c31861678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['2', '119.99200', '157.30200', '74.99700', '0.00784', '0.00007',\n",
            "       '0.00370', '0.00554', '0.01109', '0.04374', '0.42600', '0.02182',\n",
            "       '0.03130', '0.02971', '0.06545', '0.02211', '21.03300', '0.414783',\n",
            "       '0.815285', '-4.813031', '0.266482', '2.301442', '0.284654'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['status'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-7d3ef1c3d6d5>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdata_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 3- Separar o dataset em X (matriz de features) e y (coluna target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5579\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5580\u001b[0m         \"\"\"\n\u001b[0;32m-> 5581\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5582\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4787\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4788\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4829\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4830\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4831\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7071\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['status'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Exercício 3 - Dataset Penguins\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import metrics\n",
        "\n",
        "# 1- Carregar a base de dados Penguins da API do Seaborn\n",
        "data = sns.load_dataset('penguins')\n",
        "\n",
        "# 2- A base precisará ser tratada: Existem valores nulos e dados categóricos\n",
        "data = data.dropna()\n",
        "data = pd.get_dummies(data, drop_first=True)\n",
        "\n",
        "####### 3- Normalizar todas as colunas (normalizar é deixar todos os valores das colunas entre 0 e 1) --> Redes Neurais são sensíveis as diferenças escalares de valores das features\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = pd.DataFrame(scaler.fit_transform(data.drop('species', axis=1)), columns=data.columns[1:])\n",
        "\n",
        "# 4- Separar o dataset em X (matriz de features) e y (coluna target)\n",
        "X = data_scaled\n",
        "y = data['species_Adelie']\n",
        "\n",
        "# 5- Gerar as bases de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 6- Importar o modelo MLP do sklearn\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# 7- Instanciar o modelo escolhendo uma topologia para a rede, função de ativação e número de épocas de execução até que obtenha uma taxa de acerto estável\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(10,), activation='relu', max_iter=1000, random_state=42)\n",
        "\n",
        "# 8- Treinar o modelo com os dados de treinamento\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# 9- Fazer o predict com os dados de teste\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "# 10- Imprimir o percentual de acerto da base de teste\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(f\"Acurácia na base de teste: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "L0v15WSn9gS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Exercício 4 - Dataset Phoneme\n",
        "\n",
        "# 1- Carregar a base \"phoneme\"\n",
        "url = \"https://raw.githubusercontent.com/tmoura/machinelearning/master/phoneme.data\"\n",
        "\n",
        "# 2- A coluna 0 é o target\n",
        "# 3- Todas as colunas são numéricas e não possui valores nulos\n",
        "# 4- Separar o dataset em X (matriz de features) e y (coluna target)\n",
        "# 5- Gerar as bases de treinamento e teste\n",
        "# 6- Importar o modelo MLP do sklearn\n",
        "# 7- Instanciar o modelo escolhendo uma topologia para a rede, função de ativação e número de épocas de execução até que obtenha uma taxa de acerto estável\n",
        "# 8- Treinar o modelo com os dados de treinamento\n",
        "# 9- Fazer o predict com os dados de teste\n",
        "# 10- Imprimir o percentual de acerto da base de teste"
      ],
      "metadata": {
        "id": "-w3Yk186ViXm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}